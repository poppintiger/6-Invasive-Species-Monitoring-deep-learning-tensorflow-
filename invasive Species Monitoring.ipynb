{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import  h5py\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.misc import imread,imsave,imresize \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tools function to process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validate set: 229\n"
     ]
    }
   ],
   "source": [
    "def read_normalize_img(dataset,i):\n",
    "    i=str(i)\n",
    "    path='dataset/'+dataset+'/'+i+'.jpg'\n",
    "    img=imread(path)\n",
    "    img=imresize(img,(300,300))\n",
    "    #cast int to float32!!!\n",
    "    img=img.astype(np.float32)\n",
    "     #normalization(-1,1)\n",
    "    img=(img-128)/128\n",
    "    return img\n",
    "\n",
    "\n",
    "def save(dataset,file_i):\n",
    "    file_i=str(file_i)\n",
    "    file_path=\"hdf5/\"+dataset+\"/\"+file_i+\".hdf5\"\n",
    "    try:\n",
    "        if dataset=='train': \n",
    "            f=h5py.File(file_path,\"w\")\n",
    "            f[file_i]=np.array(temp_train)\n",
    "        elif dataset=='validate':\n",
    "            f=h5py.File(file_path,\"w\")\n",
    "            f[file_i]=np.array(temp_validate)\n",
    "        else:\n",
    "            f=h5py.File(file_path,\"w\")\n",
    "            f[file_i]=np.array(temp_test)\n",
    "    except Exception:\n",
    "        f.close()\n",
    "        \n",
    "                \n",
    "\n",
    "def load(dataset,file_i):\n",
    "    file_i=str(file_i)\n",
    "    file_path=\"hdf5/\"+dataset+\"/\"+file_i+\".hdf5\"\n",
    "    try:\n",
    "        f=h5py.File(file_path,\"r\")\n",
    "        #Datesets class，when slice return nmarray\n",
    "        return f[file_i]    \n",
    "    except Exception:\n",
    "        f.close()\n",
    "\n",
    "\n",
    "        \n",
    "#prepare the train data\n",
    "def load_batch_train(file_i,batch_size):\n",
    "        #load file_i\n",
    "        #hdf5 Datesets class\n",
    "        train_features=load('train',file_i)\n",
    "        return batch_train(train_features,train_labels,batch_size)\n",
    "    \n",
    "#a generator,yield the batches of feature and label for one file   \n",
    "#. \n",
    "def batch_train(features, labels, batch_size):\n",
    "    \n",
    "    \"\"\"\n",
    "    Split features and labels into batches\n",
    "    \"\"\"\n",
    "        #start：[0,batch_size,2 batch_size,.....]\n",
    "        #batch data:0-batch_size-1   batch_size_2 batch-size-1......  \n",
    "    for start in range(0, len(features), batch_size):        \n",
    "        #array slicing,not include the last element\n",
    "        #strat+batch_size should not larger than len of feature,once exceed，the end is the len\n",
    "        end = min(start + batch_size, len(features))\n",
    "        #when slice return nparray\n",
    "        yield features[start:end], labels[start:end] \n",
    "        \n",
    "  \n",
    "\n",
    "#randomly choose 10% train set as validate set\n",
    "random.seed(1)\n",
    "validate_sample_list=random.sample(range(1,2296),229)\n",
    "print('validate set:',len(validate_sample_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save train and validate features data on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have saved 1 train\n",
      "have saved 2 train\n",
      "have saved 3 train\n",
      "have saved 4 train\n",
      "have saved 5 train\n",
      "have saved 6 train\n",
      "have saved 7 train\n",
      "have saved 8 train\n",
      "have saved last train\n",
      "finished!!!\n"
     ]
    }
   ],
   "source": [
    "temp_train=[]\n",
    "temp_validate=[]\n",
    "#the num of train pics\n",
    "train_count=0\n",
    "#the num of batch\n",
    "count=1\n",
    "#save the train set in disk\n",
    "for i in range(1,2296):#2296\n",
    "    if i not in validate_sample_list:\n",
    "        train_count+=1\n",
    "        #save disk the batch of pics\n",
    "        if train_count%256==0:            \n",
    "            #write in disk,empty list and read next pic\n",
    "           # print('begin to save',count,'train')\n",
    "            save('train',count)\n",
    "            print('have saved',count,'train')\n",
    "            temp_train=[]\n",
    "            temp_train.append(read_normalize_img('train',i).tolist()) \n",
    "           # print(np.array(temp_train).shape)\n",
    "            count+=1\n",
    "                       \n",
    "        #continue appending\n",
    "        else:\n",
    "            temp_train.append(read_normalize_img('train',i).tolist())\n",
    "            \n",
    "    else:\n",
    "        temp_validate.append(read_normalize_img('train',i).tolist())\n",
    "\n",
    "#save the last batch pics to disk\n",
    "save('train',count)\n",
    "print('have saved last train')\n",
    "del temp_train\n",
    "\n",
    "#save the validate set to disk\n",
    "save('validate',1) \n",
    "del temp_validate\n",
    "\n",
    "print('finished!!!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save test features data on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished!!!\n"
     ]
    }
   ],
   "source": [
    "#save test data\n",
    "temp_test=[]\n",
    "count=1\n",
    "for i in range(1,1532):\n",
    "    if i%256==0:\n",
    "        #write in disk,empty list and read next pic\n",
    "        save('test',count)\n",
    "        temp_test=[]\n",
    "        temp_test.append(read_normalize_img('test',i).tolist())\n",
    "        count+=1\n",
    "    else:                     \n",
    "        temp_test.append(read_normalize_img('test',i).tolist())\n",
    "#save the last batch pics to disk\n",
    "save('test',count)    \n",
    "temp_test=None\n",
    "print('finished!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the labels data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read label\n",
    "import pandas as pd\n",
    "label=pd.read_csv('train_labels.csv')\n",
    "#type(label[\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validate_label:  (229, 1)\n",
      "train_label: (2066, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#validate label array\n",
    "#validate_label=label.values[(np.array(validate_sample_list)-1).tolist(),:][:,1:2].reshape(-1)\n",
    "\n",
    "validate_label_list=label[label.name.isin(validate_sample_list)].invasive.values\n",
    "\n",
    "train_label_list=label[~label.name.isin(validate_sample_list)].invasive.values\n",
    "\"\"\"\n",
    "def one_hot(label_list):\n",
    "    temp_list=[]\n",
    "    for i in label_list:\n",
    "        if i==1:\n",
    "            temp_list.append([1,0])\n",
    "        else:\n",
    "            temp_list.append([0,1])\n",
    "    return temp_list\n",
    "            \n",
    "one_hot(validate_label_list)\n",
    "\"\"\"    \n",
    "\n",
    " \n",
    "\n",
    "#list to array [batch,1]\n",
    "validate_labels=np.array(validate_label_list).reshape([-1,1])\n",
    "                                                  \n",
    "train_labels=np.array(train_label_list).reshape([-1,1])\n",
    "\n",
    "print('validate_label: ',validate_labels.shape)\n",
    "print('train_label:' ,train_labels.shape)\n",
    "#onehot encoding[batch,2]\n",
    "#def one_hot(y):\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Prepare the placeholders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#clear the graph！！！before add to it\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "# create Inputs : placeholder,placeholder define the shape uncertaity thing\n",
    "#batches of image input\n",
    "x = tf.placeholder(tf.float32,shape=(None,300,300,3),name='x')\n",
    "#batches label\n",
    "y = tf.placeholder(tf.float32,shape=(None,1),name='y')\n",
    "\n",
    "keep_prob =tf.placeholder(tf.float32,name='keep_prob')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 Encapsulate the layers: genralize to all kinds of input and output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Convolution and Max Pooling Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#input a tensor,execute the convolution and pooling,return a new tensor\n",
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides,conv_padding='SAME'):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor input(4 dimension) \n",
    "    :param conv_num_outputs: Number of output feature maps of this convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor (4 dimension) \n",
    "    \"\"\"\n",
    "    # first Max-Pooling -> finally ReLU ->output\n",
    "    #tf.truncated_normal；tf.zeros\n",
    "    #bulit parameter Variable and initialize the weight and bias of filter\n",
    "    #weight:[ksize,ksize,depth,output],all weight for all filters（or feature maps）\n",
    "    #one weight represent a filter，one out put one filter，conv_num_outputs group of weights\n",
    "    cw=tf.Variable(tf.truncated_normal([conv_ksize[0],conv_ksize[1],x_tensor.get_shape().as_list()[3],conv_num_outputs],stddev=0.04))\n",
    "    #all bias of the conv layer\n",
    "    cb=tf.Variable(tf.zeros([conv_num_outputs]))\n",
    "    #return 4 dimension output tensor[batch,conv_size1,conv_size2,conv_num_outputs]\n",
    "    conv=tf.nn.conv2d(x_tensor,cw,strides=[1,conv_strides[0],conv_strides[1],1],padding=conv_padding)\n",
    "    conv=tf.nn.bias_add(conv,cb)\n",
    "    \n",
    "    #return 4 dimension\n",
    "    output=tf.nn.relu(conv)\n",
    "    output=tf.nn.max_pool(output,ksize=[1,pool_ksize[0],pool_ksize[1],1],strides=[1,pool_strides[0],pool_strides[1],1],padding='SAME')\n",
    "    \n",
    "    return output \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flatten Layer：change a tensor from 4-D to 2-D "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#flatteb a tensor for a fully connected layer\n",
    "#finally we want[Batch Size, output],one batch one output\n",
    "#so,flatten to (Batch Size, Flattened Image Size) multiple weight:[Flattened Image Size,output] get the result!\n",
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten a x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...),  ... are the image dimensions.\n",
    "    batchsize is uncertain!!\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    shape=x_tensor.get_shape().as_list()\n",
    "    return tf.reshape(x_tensor,[-1,shape[1]*shape[2]*shape[3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fully-Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#input a 2D tensor,execute the forward pass of general neuron network，output a 2D tensor[batch,output]\n",
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    \n",
    "    fw=tf.Variable(tf.truncated_normal([x_tensor.get_shape().as_list()[1],num_outputs],stddev=0.04))\n",
    "    fb=tf.Variable(tf.zeros([num_outputs]))\n",
    "    fo=tf.add(tf.matmul(x_tensor,fw),fb)\n",
    "    output=tf.nn.relu(fo)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#input 2D tensor [batch,output]to 2D tensor[batch,logit]\n",
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    ow=tf.Variable(tf.truncated_normal([x_tensor.get_shape().as_list()[1],num_outputs],stddev=0.04))\n",
    "    ob=tf.Variable(tf.zeros([num_outputs]))\n",
    "    logit=tf.add(tf.matmul(x_tensor,ow),ob)\n",
    "    return logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Built Convolutional Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "\n",
    "\n",
    "\n",
    "#out = ceil(float(75/ float(2)))\n",
    "\n",
    "#from input x to output logit function\n",
    "def conv_net(x, keep_prob):\n",
    "    \n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "     \n",
    "\n",
    "    #built convolution layer\n",
    "    #output:[148,148]\n",
    "    conv_ksize1=(5,5)\n",
    "    conv_strides1=(2,2)  \n",
    "    \n",
    "    conv_ksize2=(3,3)\n",
    "    conv_strides2=(1,1) \n",
    "  \n",
    "    conv_ksize3=(3,3)\n",
    "    conv_strides3=(1,1)\n",
    "   \n",
    "    conv_ksize4=(3,3)\n",
    "    conv_strides=(1,)\n",
    "    \n",
    "    conv_ksize5=(3,3)\n",
    "    conv_strides=(1,1)\n",
    "    \n",
    "    pool_ksize=(2,2)\n",
    "    pool_strides=(2,2)\n",
    "    \n",
    "   # cl=conv2d_maxpool(x, 32, conv_ksize1, conv_strides1, pool_ksize, pool_strides,'SAME')\n",
    "    \n",
    "    cl=conv2d_maxpool(x, 32, conv_ksize1, conv_strides1, pool_ksize, pool_strides,'SAME')\n",
    "    #cl=tf.nn.dropout(cl,keep_prob)\n",
    "    cl=conv2d_maxpool(cl, 64, conv_ksize3, conv_strides3, pool_ksize, pool_strides,'SAME')\n",
    "   \n",
    "    cl=conv2d_maxpool(cl, 128, conv_ksize4, conv_strides3, pool_ksize, pool_strides,'SAME')\n",
    "  \n",
    "    #cl=conv2d_maxpool(cl, 512, conv_ksize5, conv_strides3, pool_ksize, pool_strides,'VALID')\n",
    "    \n",
    "    #connect fully-connceted layer\n",
    "    cl_2d=flatten(cl)\n",
    "    \n",
    "    fcl=fully_conn(cl_2d, 521)\n",
    "    fcl=tf.nn.dropout(fcl,keep_prob)\n",
    "    fcl=fully_conn(fcl, 128)\n",
    "    fcl=tf.nn.dropout(fcl,keep_prob)\n",
    "    #fcl=fully_conn(fcl, 128)\n",
    " \n",
    "    \n",
    "    #  Output Layer\n",
    "    logit=output(fcl, 1)\n",
    "    return logit\n",
    "\n",
    "# The Model！！！1 [batch,1]]\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "#logistics loss for one output\n",
    "\n",
    "y=tf.cast(y,tf.float32)\n",
    "logits=tf.cast(logits,tf.float32)\n",
    "part_1=tf.multiply(y,tf.log(tf.sigmoid(logits)))\n",
    "part_2=tf.multiply(1-y,tf.log(1-tf.sigmoid(logits)))\n",
    "#tf.subtract(tf.constant(1,dtype=tf.float32),y)\n",
    "#logistics loss function\n",
    "cost = tf.multiply(tf.constant(-1,dtype=tf.float32),tf.reduce_mean(tf.add(part_1,part_2)))\n",
    "\n",
    "#updata the weight\n",
    "#optimizer = tf.train.AdadeltaOptimizer().minimize(cost)\n",
    "#optimizer=tf.train.GradientDescentOptimizer().minimize(cost)#learning_rate=0.00001\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.0005).minimize(cost)#0.0016\n",
    "#cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "#optimizer = tf.train.AdadeltaOptimizer().minimize(cost)#learning_rate=0.0005\n",
    "\"\"\"\n",
    "解决方案1:\n",
    "\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(tf.clip_by_value(y_conv,1e-10,1.0)))  \n",
    "解决方案2:(推荐)\n",
    "\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv + 1e-10)\n",
    "\"\"\"\n",
    "#compute any features' Accuracy\n",
    "#logist,y are 2 dimension[batch,value].same shape.1 represent row dimension\n",
    "#argmax return tensor array，if the position of max logist and max label are same，predict correctly return true \n",
    "#correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "correct_pred=tf.equal(tf.greater(logits,0.5),tf.equal(y,1.0))\n",
    "#correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "#accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "#cast boolean array to float array\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the training stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#after optimizer and update the weight，run the accuracy on the same train batch data！\n",
    "#depend on logit and y\n",
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Use the present model to compute loss and validation accuracy！！！\n",
    "    Only by using x,y\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    \n",
    "    #when evaluate we use the certain model\n",
    "    cost=session.run(cost,feed_dict={x:feature_batch,y:label_batch,keep_prob:1.0})\n",
    "        \n",
    "    #use validate set!!!!\n",
    "    validate_features=load('validate',1)\n",
    "    #print('logits:',session.run(logits,feed_dict={x:validate_features,keep_prob:1.0}))        \n",
    "    #print('logits>0.5:',session.run(tf.greater(logits,0.5),feed_dict={x:validate_features,keep_prob:1.0}))\n",
    "    #print('y equal to 1:',session.run(tf.equal(y,1),feed_dict={y:validate_labels,keep_prob:1.0}))\n",
    "    #print('correct_table',session.run(correct_pred,feed_dict={x:validate_features,y:validate_labels,keep_prob:1.0}))\n",
    "    validate_accuracy=session.run(accuracy,feed_dict={x:validate_features,y:validate_labels,keep_prob:1.0})  \n",
    "    train_accuracy=session.run(accuracy,feed_dict={x:feature_batch,y:label_batch,keep_prob:1.0}) \n",
    "    \n",
    "    print('loss:{},validate_accuracy:{},train_accuracy:{}'.format(cost,validate_accuracy,train_accuracy))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "batch_size = 32\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1:loss:0.5843467712402344,validate_accuracy:0.4585152864456177,train_accuracy:0.1875\n",
      "Epoch2:loss:0.5575419664382935,validate_accuracy:0.6681222915649414,train_accuracy:0.65625\n",
      "Epoch3:loss:0.5523676872253418,validate_accuracy:0.6593886613845825,train_accuracy:0.6875\n",
      "Epoch4:loss:0.5410900712013245,validate_accuracy:0.6375545859336853,train_accuracy:0.75\n",
      "Epoch5:loss:0.5293526649475098,validate_accuracy:0.6462882161140442,train_accuracy:0.78125\n",
      "Epoch6:loss:0.5394490361213684,validate_accuracy:0.7030567526817322,train_accuracy:0.78125\n",
      "Epoch7:loss:0.542778730392456,validate_accuracy:0.6768559217453003,train_accuracy:0.71875\n",
      "Epoch8:loss:0.5045607089996338,validate_accuracy:0.7030567526817322,train_accuracy:0.75\n",
      "Epoch9:"
     ]
    }
   ],
   "source": [
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    \n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    \n",
    "    for epoch in range(1,epochs+1):\n",
    "        print('Epoch'+str(epoch)+':',end='')\n",
    "         # Loop over all files\n",
    "        n_files = 7\n",
    "        for i in range(1, n_files + 1):\n",
    "            for feature_batch,label_batch in load_batch_train(i,batch_size):                \n",
    "                sess.run(optimizer,feed_dict={x:feature_batch,y:label_batch,keep_prob:keep_probability})\n",
    "                \n",
    "        #after train,test the present model \n",
    "        print_stats(sess, feature_batch, label_batch, cost, accuracy)\n",
    "\n",
    "    \n",
    "      # Save Model\n",
    "      #loss func value 跟 validate_accuracy,没有直接的关系，不需要太关注它\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, 'saved_model/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Built Convolutional Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1 checking the model on one batch\n",
    "#2 spend litte time to iterate  the model for a better accuracy\n",
    "\"\"\"\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(1,epochs+1):\n",
    "        for feature_batch,label_batch in load_batch_train(1,batch_size):\n",
    "            train(sess, optimizer, keep_probability, feature_batch, label_batch)\n",
    "        print('Epoch'+str(epoch)+':')\n",
    "        #after train,test the present model \n",
    "        print_stats(sess, feature_batch, label_batch, cost, accuracy)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
